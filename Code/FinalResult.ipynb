{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e5bb77-abf4-40a3-a8a5-820eedfc0c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efb0c350-42c0-4b20-b6d6-ab56953e5a8d",
   "metadata": {},
   "source": [
    "## Importing Libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5fbd4179-9924-49af-9093-6e29e0272e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Imports:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk import word_tokenize\n",
    "from nltk import pos_tag\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# Preprocessing Imports\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn.preprocessing as preproc\n",
    "from sklearn.feature_extraction import text\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from random import shuffle\n",
    "import random\n",
    "random.seed(123)\n",
    "from utils.syntactic_similarity_measures import SyntacticMeasures\n",
    "from utils.lesk_algorithm import Lesk\n",
    "from utils.semantic_similarity_measures import SemanticMeasures\n",
    "from utils.wordnet import GetWordnetPos\n",
    "from pre_processor import Preprocess\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Plotting Imports:\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Model Imports:\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from random import shuffle\n",
    "import random\n",
    "random.seed(123)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.model_selection\n",
    "from sklearn import svm\n",
    "\n",
    "# Metrics Import\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272bcd78-f39b-44de-90de-4d43b0d85352",
   "metadata": {},
   "source": [
    "## Preprocessing Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4da05beb-8e68-4af9-b7b6-438cb394274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter_cosine_similarity(token1, token2):\n",
    "    c1 = Counter(token1)\n",
    "    c2 = Counter(token2)\n",
    "    terms = set(c1).union(c2)\n",
    "    dotprod = sum(c1.get(k, 0) * c2.get(k, 0) for k in terms)\n",
    "    magA = math.sqrt(sum(c1.get(k, 0)**2 for k in terms))\n",
    "    magB = math.sqrt(sum(c2.get(k, 0)**2 for k in terms))\n",
    "    return dotprod / (magA * magB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b1e45c67-b749-4828-bd12-b2ad29619788",
   "metadata": {},
   "outputs": [],
   "source": [
    "def length_similarity(token1, token2):\n",
    "    c1 = Counter(token1)\n",
    "    c2 = Counter(token2)\n",
    "    lenc1 = sum(iter(c1.values()))\n",
    "    lenc2 = sum(iter(c2.values()))\n",
    "    lengthSim = min(lenc1, lenc2) / float(max(lenc1, lenc2))\n",
    "    return lengthSim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0b4ab6ae-f8b5-417d-bc98-15f66bc9f09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap_score(token1, token2):\n",
    "    c1 = Counter(token1)\n",
    "    c2 = Counter(token2)\n",
    "    lenc1 = sum(iter(c1.values()))\n",
    "    lenc2 = sum(iter(c2.values()))\n",
    "    overlappingtermsCount = sum(((c1)&(c2)).values())\n",
    "    overlap_score = abs((overlappingtermsCount/lenc1) - (overlappingtermsCount/lenc2))\n",
    "    return overlap_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e8628c9b-3754-402b-8cdc-0c1180a91b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlap2_score(token1, token2):\n",
    "    c1 = Counter(token1)\n",
    "    c2 = Counter(token2)\n",
    "    lenc1 = sum(iter(c1.values()))\n",
    "    lenc2 = sum(iter(c2.values()))\n",
    "    overlappingtermsCount = sum(((c1)&(c2)).values())\n",
    "    overlap2_score = (overlappingtermsCount/(lenc1+lenc2))\n",
    "    return overlap2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ef9fbe8a-f508-4b80-91bc-7f4aad355225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score(lengthSim,cosine_score):\n",
    "    return lengthSim*cosine_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "24f1eb36-a9f5-4998-a506-d6eec07c2317",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(token1, token2):\n",
    "        \"\"\" compute cosine similarity \"\"\"\n",
    "        cosine_similarity = SyntacticMeasures.getCosineSimilarity(token1,token2)\n",
    "        return cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d7a2f117-6098-46c7-a209-e35272f01911",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jaccard_similarity(token1, token2):\n",
    "        \"\"\" compute jaccard similarity\"\"\"\n",
    "        jaccard_similarity = SyntacticMeasures.normal_jaccard_distance(token1,token2)\n",
    "        return jaccard_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "55147d2e-aa5c-48ff-a29f-ef23b9f89813",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lemma_jaccard_similarity(lemma1, lemma2):\n",
    "        \"\"\" compute lemma jaccard similarity\"\"\"\n",
    "        lemma_jaccard_similarity = SyntacticMeasures.lemma_jaccard_distance(lemma1,lemma2)\n",
    "        return lemma_jaccard_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e44af08c-0182-4bae-9a34-d329fc1dcf5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def overall_similarity_combined(token1, token2, lemma1, lemma2):\n",
    "        \"\"\" calculate combined similarity \"\"\"\n",
    "\n",
    "        R1 = compute_cosine_similarity(token1,token2)\n",
    "        R2 = compute_jaccard_similarity(token1,token2)\n",
    "        R3 = compute_lemma_jaccard_similarity(lemma1,lemma2)\n",
    "        R = (R1+R2+R3)/3\n",
    "        return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e83f5f75-482b-47a4-bda7-0fda4e327f07",
   "metadata": {},
   "outputs": [],
   "source": [
    " def get_lesk(ques):\n",
    "        \"\"\" get each word meaning out of the given question\"\"\"\n",
    "        lesk_obj = Lesk(ques)\n",
    "        sentence_means = []\n",
    "        for word in ques:\n",
    "            sentence_means.append(lesk_obj.lesk(word, ques))\n",
    "        return sentence_means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "28fe70cc-70fa-4b02-aa9b-d79413eae477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_similarities(token1, token2):\n",
    "    sentence_means1 = get_lesk(token1)\n",
    "    sentence_means2 = get_lesk(token2)\n",
    "    \n",
    "    RWUP = SemanticMeasures.computeWup(sentence_means1, sentence_means2)\n",
    "    OverallWUP = SemanticMeasures.overallSim(sentence_means1, sentence_means2, RWUP)\n",
    "    RSIM = SemanticMeasures.computePath(sentence_means1, sentence_means2)\n",
    "    OverallSIM = SemanticMeasures.overallSim(sentence_means1, sentence_means2, RSIM)\n",
    "    RCOMBINED = (RWUP + RSIM)/2\n",
    "    OverallCombined = SemanticMeasures.overallSim(sentence_means1, sentence_means2, RCOMBINED)\n",
    "    \n",
    "    score_list = [OverallWUP,OverallSIM, OverallCombined]\n",
    "    return score_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ae005034-b6cc-4d70-a30d-5ed530b921c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thePreprocessorNoLemma(token1):\n",
    "    processor = Preprocess(token1)\n",
    "    token = processor.preprocess_without_lemma()\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e6cb6971-04f5-467f-b6ab-5f98f78c391c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def thePreprocessorLemma(lemma1):\n",
    "    processor = Preprocess(lemma1)\n",
    "    lemma_token = processor.preprocess_with_lemma()\n",
    "    return lemma_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8d468c5-ea11-4008-ac89-8cd905461a11",
   "metadata": {},
   "source": [
    "## Preprocessing of Training Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "31b3ebb2-938b-476f-9d2b-866782ee94a0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 're' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-a72f78853d13>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#Text Cleaning Features:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mtrain_df3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text_Cleaned1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthePreprocessorNoLemma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_df3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentence1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mtrain_df3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text_Cleaned2'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthePreprocessorNoLemma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_df3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentence2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mtrain_df3\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lemmatized_text1'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthePreprocessorLemma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_df3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentence1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-50-62845e9bb05f>\u001b[0m in \u001b[0;36mthePreprocessorNoLemma\u001b[1;34m(token1)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mthePreprocessorNoLemma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mprocessor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtoken1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mtoken\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocess_without_lemma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mtoken\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\MLMidTerm\\Code\\pre_processor.py\u001b[0m in \u001b[0;36mpreprocess_without_lemma\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     76\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mnew_tokens\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 78\u001b[1;33m     \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess_without_lemma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     79\u001b[0m         \u001b[1;31m# tokenization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[0mtokens\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\MLMidTerm\\Code\\pre_processor.py\u001b[0m in \u001b[0;36mget_tokens\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_tokens\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;34m\"\"\" return list of tokens\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[1;32mimport\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[1;31m# Format words and remove unwanted characters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentence\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr'https?:\\/\\/.*[\\r\\n]*'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMULTILINE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 're' is not defined"
     ]
    }
   ],
   "source": [
    "train_df3 = pd.read_csv(\"../Data/train_with_label.txt\", delimiter = \"r'\\t\", header = None, engine = 'python')\n",
    "train_df3 = train_df3[0].str.split(\"\\t\", expand=True)\n",
    "train_df3 = train_df3.rename(columns={0: \"id\", 1: \"sentence1\", 2: \"sentence2\", 3: \"classification\"})\n",
    "train_df3[\"classification\"] = pd.to_numeric(train_df3[\"classification\"])\n",
    "train_df3.drop_duplicates(inplace = True)\n",
    "train_df3\n",
    "\n",
    "#Text Cleaning Features:\n",
    "train_df3['Text_Cleaned1'] = list(map(thePreprocessorNoLemma, train_df3.sentence1))\n",
    "train_df3['Text_Cleaned2'] = list(map(thePreprocessorNoLemma, train_df3.sentence2))\n",
    "train_df3['lemmatized_text1'] = list(map(thePreprocessorLemma, train_df3.sentence1))\n",
    "train_df3['lemmatized_text2'] = list(map(thePreprocessorLemma, train_df3.sentence2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb49121-9f72-4a5b-9101-2eec13d95449",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Syntactic Features:\n",
    "train_df3['cosine_similarity_score'] = list(map(counter_cosine_similarity, train_df3.Text_Cleaned1, train_df3.Text_Cleaned2))\n",
    "train_df3['length_similarity'] = list(map(length_similarity, train_df3.Text_Cleaned1, train_df3.Text_Cleaned2))\n",
    "train_df3['overlap_score'] = list(map(overlap_score, train_df3.Text_Cleaned1, train_df3.Text_Cleaned2))\n",
    "train_df3['overlap2_score'] = list(map(overlap2_score, train_df3.Text_Cleaned1, train_df3.Text_Cleaned2))\n",
    "train_df3['cosine/length_ratio'] = list(map(similarity_score, train_df3.length_similarity, train_df3.cosine_similarity_score))\n",
    "train_df3['cosine_similarity_score2'] = list(map(compute_cosine_similarity, train_df3.Text_Cleaned1, train_df3.Text_Cleaned2))\n",
    "train_df3['jaccard_similarity_score'] = list(map(compute_jaccard_similarity, train_df3.Text_Cleaned1, train_df3.Text_Cleaned2))\n",
    "train_df3['lemma_jaccard_score'] = list(map(compute_lemma_jaccard_similarity, train_df3.lemmatized_text1, train_df3.lemmatized_text2))\n",
    "train_df3['overall_sim_score'] = list(map(overall_similarity_combined, train_df3.Text_Cleaned1, train_df3.Text_Cleaned2, train_df3.lemmatized_text1, train_df3.lemmatized_text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb2363-50e4-4061-900a-415bf3324a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Semantic Features:\n",
    "pdf1 = train_df3.iloc[:800]\n",
    "pdf2 = train_df3.iloc[800:1600]\n",
    "pdf3 = train_df3.iloc[1600:2400]\n",
    "pdf4 = train_df3.iloc[2400:3200]\n",
    "pdf5 = train_df3.iloc[3200:4077]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "534e17c0-c71f-4c93-80e3-c6d6166b8974",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf1['scores'] = list(map(semantic_similarities, pdf1.lemmatized_text1, pdf1.lemmatized_text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "a594d2bf-32b5-42b6-8df8-dab64acde062",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf2['scores'] = list(map(semantic_similarities, pdf2.lemmatized_text1, pdf2.lemmatized_text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "28581c7a-d39b-4c91-af6b-6049ba424006",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf3['scores'] = list(map(semantic_similarities, pdf3.lemmatized_text1, pdf3.lemmatized_text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "9e7987fc-7b70-4b79-84ef-5d110decdd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf4['scores'] = list(map(semantic_similarities, pdf4.lemmatized_text1, pdf4.lemmatized_text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0b4b595c-cacb-40a7-9e17-283c309fa3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf5['scores'] = list(map(semantic_similarities, pdf5.lemmatized_text1, pdf5.lemmatized_text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "c7c29e5b-24a1-4fa5-a7cd-cbe8c5dcb6e1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>classification</th>\n",
       "      <th>Text_Cleaned1</th>\n",
       "      <th>Text_Cleaned2</th>\n",
       "      <th>lemmatized_text1</th>\n",
       "      <th>lemmatized_text2</th>\n",
       "      <th>cosine_similarity_score</th>\n",
       "      <th>length_similarity</th>\n",
       "      <th>overlap_score</th>\n",
       "      <th>overlap2_score</th>\n",
       "      <th>cosine/length_ratio</th>\n",
       "      <th>cosine_similarity_score2</th>\n",
       "      <th>jaccard_similarity_score</th>\n",
       "      <th>lemma_jaccard_score</th>\n",
       "      <th>overall_sim_score</th>\n",
       "      <th>overall_similarity_path_semantic</th>\n",
       "      <th>overall_similarity_wup_semantic</th>\n",
       "      <th>overall_similarity_combined_semantic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_id_0</td>\n",
       "      <td>The Democratic candidates also began announcin...</td>\n",
       "      <td>The Democratic candidates also began announcin...</td>\n",
       "      <td>1</td>\n",
       "      <td>[The, Democratic, candidates, also, began, ann...</td>\n",
       "      <td>[The, Democratic, candidates, also, began, ann...</td>\n",
       "      <td>[democratic, candidate, also, begin, announce,...</td>\n",
       "      <td>[democratic, candidate, also, begin, announce,...</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.441176</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.789474</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.855750</td>\n",
       "      <td>0.906994</td>\n",
       "      <td>0.902778</td>\n",
       "      <td>0.904886</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_id_1</td>\n",
       "      <td>The woman was exposed to the SARS virus while ...</td>\n",
       "      <td>The woman was exposed to the SARS virus while ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[The, woman, exposed, SARS, virus, hospital, h...</td>\n",
       "      <td>[The, woman, exposed, SARS, virus, hospital, h...</td>\n",
       "      <td>[woman, expose, sars, virus, hospital, health,...</td>\n",
       "      <td>[woman, expose, sars, virus, hospital, health-...</td>\n",
       "      <td>0.721688</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.077778</td>\n",
       "      <td>0.368421</td>\n",
       "      <td>0.649519</td>\n",
       "      <td>0.962250</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.732871</td>\n",
       "      <td>0.914216</td>\n",
       "      <td>0.921569</td>\n",
       "      <td>0.917892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_id_2</td>\n",
       "      <td>He said the problem needs to be corrected befo...</td>\n",
       "      <td>He said the prob lem needs to be corrected bef...</td>\n",
       "      <td>1</td>\n",
       "      <td>[He, said, problem, needs, corrected, space, s...</td>\n",
       "      <td>[He, said, prob, lem, needs, corrected, space,...</td>\n",
       "      <td>[say, problem, need, correct, space, shuttle, ...</td>\n",
       "      <td>[say, prob, lem, need, correct, space, shuttle...</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.538462</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.724942</td>\n",
       "      <td>0.792256</td>\n",
       "      <td>0.664021</td>\n",
       "      <td>0.721825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_id_3</td>\n",
       "      <td>A representative for Phoenix-based U-Haul decl...</td>\n",
       "      <td>Anthony Citrano , a representative for WhenU ,...</td>\n",
       "      <td>0</td>\n",
       "      <td>[A, representative, Phoenix-based, U-Haul, dec...</td>\n",
       "      <td>[Anthony, Citrano, representative, WhenU, decl...</td>\n",
       "      <td>[representative, phoenix-based, u-haul, declin...</td>\n",
       "      <td>[anthony, citrano, representative, whenu, decl...</td>\n",
       "      <td>0.455842</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>0.207792</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.290081</td>\n",
       "      <td>0.797724</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.463710</td>\n",
       "      <td>0.613296</td>\n",
       "      <td>0.523529</td>\n",
       "      <td>0.561639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_id_4</td>\n",
       "      <td>The biggest threat to order seemed to be looti...</td>\n",
       "      <td>The biggest threat to order seemed to be looti...</td>\n",
       "      <td>1</td>\n",
       "      <td>[The, biggest, threat, order, seemed, looting,...</td>\n",
       "      <td>[The, biggest, threat, order, seemed, looting,...</td>\n",
       "      <td>[big, threat, order, seem, loot, crime, includ...</td>\n",
       "      <td>[big, threat, order, seem, loot, crime, includ...</td>\n",
       "      <td>0.739940</td>\n",
       "      <td>0.764706</td>\n",
       "      <td>0.199095</td>\n",
       "      <td>0.366667</td>\n",
       "      <td>0.565837</td>\n",
       "      <td>0.874475</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.669659</td>\n",
       "      <td>0.771062</td>\n",
       "      <td>0.737500</td>\n",
       "      <td>0.754281</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4072</th>\n",
       "      <td>train_id_4072</td>\n",
       "      <td>Axelrod died in his sleep of heart failure , s...</td>\n",
       "      <td>Axelrod died of heart failure while asleep at ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Axelrod, died, sleep, heart, failure, said, d...</td>\n",
       "      <td>[Axelrod, died, heart, failure, asleep, Los, A...</td>\n",
       "      <td>[axelrod, die, sleep, heart, failure, say, dau...</td>\n",
       "      <td>[axelrod, die, heart, failure, asleep, los, an...</td>\n",
       "      <td>0.805823</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.604367</td>\n",
       "      <td>0.886405</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.684357</td>\n",
       "      <td>0.567460</td>\n",
       "      <td>0.493651</td>\n",
       "      <td>0.518707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4073</th>\n",
       "      <td>train_id_4073</td>\n",
       "      <td>Saddam 's other son , Odai , surrendered Frida...</td>\n",
       "      <td>Hussein 's other son , Uday , surrendered yest...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Saddam, 's, son, Odai, surrendered, Friday, A...</td>\n",
       "      <td>[Hussein, 's, son, Uday, surrendered, yesterda...</td>\n",
       "      <td>[saddam, 's, son, odai, surrender, friday, ame...</td>\n",
       "      <td>[hussein, 's, son, uday, surrender, yesterday,...</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.466667</td>\n",
       "      <td>0.644444</td>\n",
       "      <td>0.848856</td>\n",
       "      <td>0.847222</td>\n",
       "      <td>0.847222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4074</th>\n",
       "      <td>train_id_4074</td>\n",
       "      <td>If Senator Clinton does decide to run in 2008 ...</td>\n",
       "      <td>If Mrs Clinton does decide to contest the 2008...</td>\n",
       "      <td>1</td>\n",
       "      <td>[If, Senator, Clinton, decide, run, 2008, anno...</td>\n",
       "      <td>[If, Mrs, Clinton, decide, contest, 2008, elec...</td>\n",
       "      <td>[senator, clinton, decide, run, 2008, announce...</td>\n",
       "      <td>[mr, clinton, decide, contest, 2008, election,...</td>\n",
       "      <td>0.800641</td>\n",
       "      <td>0.923077</td>\n",
       "      <td>0.064103</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.739053</td>\n",
       "      <td>0.960769</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.756764</td>\n",
       "      <td>0.834569</td>\n",
       "      <td>0.789855</td>\n",
       "      <td>0.809704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4075</th>\n",
       "      <td>train_id_4075</td>\n",
       "      <td>The Iranian refugee who sewed up his eyes , li...</td>\n",
       "      <td>An Iranian Kurd who stitched up his eyes , lip...</td>\n",
       "      <td>1</td>\n",
       "      <td>[The, Iranian, refugee, sewed, eyes, lips, ear...</td>\n",
       "      <td>[An, Iranian, Kurd, stitched, eyes, lips, ears...</td>\n",
       "      <td>[iranian, refugee, sew, eye, lip, ear, protest...</td>\n",
       "      <td>[iranian, kurd, stitch, eye, lip, ear, protest...</td>\n",
       "      <td>0.518875</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.481812</td>\n",
       "      <td>0.963624</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.567504</td>\n",
       "      <td>0.700889</td>\n",
       "      <td>0.562857</td>\n",
       "      <td>0.631617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4076</th>\n",
       "      <td>train_id_4076</td>\n",
       "      <td>Gemstar 's shares gathered up 2.6 percent , ad...</td>\n",
       "      <td>Gemstar shares moved higher on the news , clos...</td>\n",
       "      <td>1</td>\n",
       "      <td>[Gemstar, 's, shares, gathered, 2.6, percent, ...</td>\n",
       "      <td>[Gemstar, shares, moved, higher, news, closing...</td>\n",
       "      <td>[gemstar, 's, share, gather, 2.6, percent, add...</td>\n",
       "      <td>[gemstar, share, move, higher, news, close, 2....</td>\n",
       "      <td>0.476731</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.045455</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.433392</td>\n",
       "      <td>0.953463</td>\n",
       "      <td>0.312500</td>\n",
       "      <td>0.400000</td>\n",
       "      <td>0.555321</td>\n",
       "      <td>0.483069</td>\n",
       "      <td>0.434921</td>\n",
       "      <td>0.454762</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4077 rows × 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                          sentence1  \\\n",
       "0        train_id_0  The Democratic candidates also began announcin...   \n",
       "1        train_id_1  The woman was exposed to the SARS virus while ...   \n",
       "2        train_id_2  He said the problem needs to be corrected befo...   \n",
       "3        train_id_3  A representative for Phoenix-based U-Haul decl...   \n",
       "4        train_id_4  The biggest threat to order seemed to be looti...   \n",
       "...             ...                                                ...   \n",
       "4072  train_id_4072  Axelrod died in his sleep of heart failure , s...   \n",
       "4073  train_id_4073  Saddam 's other son , Odai , surrendered Frida...   \n",
       "4074  train_id_4074  If Senator Clinton does decide to run in 2008 ...   \n",
       "4075  train_id_4075  The Iranian refugee who sewed up his eyes , li...   \n",
       "4076  train_id_4076  Gemstar 's shares gathered up 2.6 percent , ad...   \n",
       "\n",
       "                                              sentence2  classification  \\\n",
       "0     The Democratic candidates also began announcin...               1   \n",
       "1     The woman was exposed to the SARS virus while ...               1   \n",
       "2     He said the prob lem needs to be corrected bef...               1   \n",
       "3     Anthony Citrano , a representative for WhenU ,...               0   \n",
       "4     The biggest threat to order seemed to be looti...               1   \n",
       "...                                                 ...             ...   \n",
       "4072  Axelrod died of heart failure while asleep at ...               1   \n",
       "4073  Hussein 's other son , Uday , surrendered yest...               1   \n",
       "4074  If Mrs Clinton does decide to contest the 2008...               1   \n",
       "4075  An Iranian Kurd who stitched up his eyes , lip...               1   \n",
       "4076  Gemstar shares moved higher on the news , clos...               1   \n",
       "\n",
       "                                          Text_Cleaned1  \\\n",
       "0     [The, Democratic, candidates, also, began, ann...   \n",
       "1     [The, woman, exposed, SARS, virus, hospital, h...   \n",
       "2     [He, said, problem, needs, corrected, space, s...   \n",
       "3     [A, representative, Phoenix-based, U-Haul, dec...   \n",
       "4     [The, biggest, threat, order, seemed, looting,...   \n",
       "...                                                 ...   \n",
       "4072  [Axelrod, died, sleep, heart, failure, said, d...   \n",
       "4073  [Saddam, 's, son, Odai, surrendered, Friday, A...   \n",
       "4074  [If, Senator, Clinton, decide, run, 2008, anno...   \n",
       "4075  [The, Iranian, refugee, sewed, eyes, lips, ear...   \n",
       "4076  [Gemstar, 's, shares, gathered, 2.6, percent, ...   \n",
       "\n",
       "                                          Text_Cleaned2  \\\n",
       "0     [The, Democratic, candidates, also, began, ann...   \n",
       "1     [The, woman, exposed, SARS, virus, hospital, h...   \n",
       "2     [He, said, prob, lem, needs, corrected, space,...   \n",
       "3     [Anthony, Citrano, representative, WhenU, decl...   \n",
       "4     [The, biggest, threat, order, seemed, looting,...   \n",
       "...                                                 ...   \n",
       "4072  [Axelrod, died, heart, failure, asleep, Los, A...   \n",
       "4073  [Hussein, 's, son, Uday, surrendered, yesterda...   \n",
       "4074  [If, Mrs, Clinton, decide, contest, 2008, elec...   \n",
       "4075  [An, Iranian, Kurd, stitched, eyes, lips, ears...   \n",
       "4076  [Gemstar, shares, moved, higher, news, closing...   \n",
       "\n",
       "                                       lemmatized_text1  \\\n",
       "0     [democratic, candidate, also, begin, announce,...   \n",
       "1     [woman, expose, sars, virus, hospital, health,...   \n",
       "2     [say, problem, need, correct, space, shuttle, ...   \n",
       "3     [representative, phoenix-based, u-haul, declin...   \n",
       "4     [big, threat, order, seem, loot, crime, includ...   \n",
       "...                                                 ...   \n",
       "4072  [axelrod, die, sleep, heart, failure, say, dau...   \n",
       "4073  [saddam, 's, son, odai, surrender, friday, ame...   \n",
       "4074  [senator, clinton, decide, run, 2008, announce...   \n",
       "4075  [iranian, refugee, sew, eye, lip, ear, protest...   \n",
       "4076  [gemstar, 's, share, gather, 2.6, percent, add...   \n",
       "\n",
       "                                       lemmatized_text2  \\\n",
       "0     [democratic, candidate, also, begin, announce,...   \n",
       "1     [woman, expose, sars, virus, hospital, health-...   \n",
       "2     [say, prob, lem, need, correct, space, shuttle...   \n",
       "3     [anthony, citrano, representative, whenu, decl...   \n",
       "4     [big, threat, order, seem, loot, crime, includ...   \n",
       "...                                                 ...   \n",
       "4072  [axelrod, die, heart, failure, asleep, los, an...   \n",
       "4073  [hussein, 's, son, uday, surrender, yesterday,...   \n",
       "4074  [mr, clinton, decide, contest, 2008, election,...   \n",
       "4075  [iranian, kurd, stitch, eye, lip, ear, protest...   \n",
       "4076  [gemstar, share, move, higher, news, close, 2....   \n",
       "\n",
       "      cosine_similarity_score  length_similarity  overlap_score  \\\n",
       "0                    0.882353           1.000000       0.000000   \n",
       "1                    0.721688           0.900000       0.077778   \n",
       "2                    0.700000           1.000000       0.000000   \n",
       "3                    0.455842           0.636364       0.207792   \n",
       "4                    0.739940           0.764706       0.199095   \n",
       "...                       ...                ...            ...   \n",
       "4072                 0.805823           0.750000       0.222222   \n",
       "4073                 0.714286           1.000000       0.000000   \n",
       "4074                 0.800641           0.923077       0.064103   \n",
       "4075                 0.518875           0.928571       0.038462   \n",
       "4076                 0.476731           0.909091       0.045455   \n",
       "\n",
       "      overlap2_score  cosine/length_ratio  cosine_similarity_score2  \\\n",
       "0           0.441176             0.882353                  1.000000   \n",
       "1           0.368421             0.649519                  0.962250   \n",
       "2           0.350000             0.700000                  1.000000   \n",
       "3           0.222222             0.290081                  0.797724   \n",
       "4           0.366667             0.565837                  0.874475   \n",
       "...              ...                  ...                       ...   \n",
       "4072        0.380952             0.604367                  0.886405   \n",
       "4073        0.333333             0.714286                  1.000000   \n",
       "4074        0.400000             0.739053                  0.960769   \n",
       "4075        0.259259             0.481812                  0.963624   \n",
       "4076        0.238095             0.433392                  0.953463   \n",
       "\n",
       "      jaccard_similarity_score  lemma_jaccard_score  overall_sim_score  \\\n",
       "0                     0.789474             0.777778           0.855750   \n",
       "1                     0.636364             0.600000           0.732871   \n",
       "2                     0.538462             0.636364           0.724942   \n",
       "3                     0.285714             0.307692           0.463710   \n",
       "4                     0.578947             0.555556           0.669659   \n",
       "...                        ...                  ...                ...   \n",
       "4072                  0.583333             0.583333           0.684357   \n",
       "4073                  0.466667             0.466667           0.644444   \n",
       "4074                  0.666667             0.642857           0.756764   \n",
       "4075                  0.350000             0.388889           0.567504   \n",
       "4076                  0.312500             0.400000           0.555321   \n",
       "\n",
       "      overall_similarity_path_semantic  overall_similarity_wup_semantic  \\\n",
       "0                             0.906994                         0.902778   \n",
       "1                             0.914216                         0.921569   \n",
       "2                             0.792256                         0.664021   \n",
       "3                             0.613296                         0.523529   \n",
       "4                             0.771062                         0.737500   \n",
       "...                                ...                              ...   \n",
       "4072                          0.567460                         0.493651   \n",
       "4073                          0.848856                         0.847222   \n",
       "4074                          0.834569                         0.789855   \n",
       "4075                          0.700889                         0.562857   \n",
       "4076                          0.483069                         0.434921   \n",
       "\n",
       "      overall_similarity_combined_semantic  \n",
       "0                                 0.904886  \n",
       "1                                 0.917892  \n",
       "2                                 0.721825  \n",
       "3                                 0.561639  \n",
       "4                                 0.754281  \n",
       "...                                    ...  \n",
       "4072                              0.518707  \n",
       "4073                              0.847222  \n",
       "4074                              0.809704  \n",
       "4075                              0.631617  \n",
       "4076                              0.454762  \n",
       "\n",
       "[4077 rows x 20 columns]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df3 = pdf1.append(pdf2).append(pdf3).append(pdf4).append(pdf5)\n",
    "train_df3[['overall_similarity_path_semantic', 'overall_similarity_wup_semantic', 'overall_similarity_combined_semantic']] = pd.DataFrame(train_df3.scores.tolist(), index= train_df3.index)\n",
    "train_df3.drop(['scores'], axis=1, inplace=True)\n",
    "train_df3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
