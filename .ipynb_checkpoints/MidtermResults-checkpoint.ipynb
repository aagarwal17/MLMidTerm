{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3c779fe-5098-4633-aa57-dff0966c79aa",
   "metadata": {},
   "source": [
    "# 4526 Midterm Solution:\n",
    "Arun Agarwal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1812d29-2aa1-4913-a897-72d31a7e5ca1",
   "metadata": {},
   "source": [
    "## Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "24eb6c75-29b2-4a6b-a53b-1cdf3ed5d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Basic Imports:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "# Plotting Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "from sklearn import tree\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn import datasets\n",
    "\n",
    "# Preprocessing Imports\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import sklearn.preprocessing as preproc\n",
    "from sklearn.feature_extraction import text\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from random import shuffle\n",
    "import random\n",
    "random.seed(123)\n",
    "\n",
    "# Model Imports:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.model_selection\n",
    "\n",
    "# Metrics Import\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.metrics import classification_report \n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8457547a-9028-4b01-9e17-41cf692ab265",
   "metadata": {},
   "source": [
    "## Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ee6ae63-8d54-4695-9714-b720b9a822a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_baseline_results_for_single_classifier(data, target_col, clf, filename):\n",
    "    X, y = data.drop(target_col, axis=1), data[target_col]\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X,y,test_size=0.2)\n",
    "    clf = clf.fit(X_tr, y_tr)\n",
    "    y_pred = clf.predict(X_te)\n",
    "    report = classification_report(y_te, y_pred, output_dict=True)\n",
    "    pd.DataFrame(report).transpose().to_csv(f'../Data/baseline_results/{filename}.csv')\n",
    "    return report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6580373f-4389-4ee7-bf3d-216529c6edf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_error(model, data, target_name, loss='mse'):\n",
    "    X, y = data.drop(target_name, axis=1), data[target_name]\n",
    "    X_tr, X_te, y_tr, y_te = train_test_split(X,y,test_size=0.2)\n",
    "    avg_expected_loss, avg_bias, avg_var = bias_variance_decomp(\n",
    "        model, X_tr.values, y_tr.values, X_te.values, y_te.values, \n",
    "        loss=loss,\n",
    "        random_seed=42)\n",
    "    print(\"Average Expected Loss: \", avg_expected_loss)\n",
    "    print(\"Average Bias: \", avg_bias)\n",
    "    print(\"Average Variance: \", avg_var)\n",
    "    \n",
    "    return avg_expected_loss, avg_bias, avg_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d668f70-a4f2-465e-a9f8-9092f8bdb532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5565b633-98eb-4f71-b892-fa087771a22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        text = text.split()\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    # Tokenize each word\n",
    "    text =  nltk.WordPunctTokenizer().tokenize(text)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40bbc85b-8576-48ce-8554-ffc1bf3217c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter_cosine_similarity(c1, c2):\n",
    "    terms = set(c1).union(c2)\n",
    "    dotprod = sum(c1.get(k, 0) * c2.get(k, 0) for k in terms)\n",
    "    magA = math.sqrt(sum(c1.get(k, 0)**2 for k in terms))\n",
    "    magB = math.sqrt(sum(c2.get(k, 0)**2 for k in terms))\n",
    "    return dotprod / (magA * magB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cf84fa78-526d-4f26-89b5-76cc45ac4791",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_logistic_classify(X_tr, y_tr, X_test, y_test, description, _C=1.0 ):\n",
    "    model = LogisticRegression(C=_C).fit(X_tr, y_tr)\n",
    "    y_pred = model.predict(X_test)\n",
    "    score = model.score(X_test, y_test)\n",
    "    print('Test Score with', description, 'features', score)\n",
    "    report = classification_report(y_test, y_pred,output_dict=True)\n",
    "    pd.DataFrame(report).transpose().to_csv(f'LogisticRegression{description}.csv')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c214ef1-dc63-4133-938c-d821af5143bd",
   "metadata": {},
   "source": [
    "## Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "c16a5ae6-fca1-4768-93ca-891291761816",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train_with_label.txt\", delimiter = \"r'\\t\", header = None, engine = 'python')\n",
    "train_df = train_df[0].str.split(\"\\t\", expand=True)\n",
    "train_df = train_df.rename(columns={0: \"id\", 1: \"sentence1\", 2: \"sentence2\", 3: \"classification\"})\n",
    "train_df[\"classification\"] = pd.to_numeric(train_df[\"classification\"])\n",
    "train_df.drop_duplicates(inplace = True)\n",
    "\n",
    "lemm = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "train_df['Text_Cleaned1'] = list(map(clean_text, train_df.sentence1))\n",
    "train_df['lemmatized_text1'] = list(map(lambda word:list(map(lemm.lemmatize, word)),train_df.Text_Cleaned1))\n",
    "train_df['Text_Cleaned2'] = list(map(clean_text, train_df.sentence2))\n",
    "train_df['lemmatized_text2'] = list(map(lambda word:list(map(lemm.lemmatize, word)),train_df.Text_Cleaned2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "36843d82-fe03-47da-8e33-7c7502783265",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>classification</th>\n",
       "      <th>Text_Cleaned1</th>\n",
       "      <th>lemmatized_text1</th>\n",
       "      <th>Text_Cleaned2</th>\n",
       "      <th>lemmatized_text2</th>\n",
       "      <th>cosine_similarity_score</th>\n",
       "      <th>overall_similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_id_0</td>\n",
       "      <td>The Democratic candidates also began announcin...</td>\n",
       "      <td>The Democratic candidates also began announcin...</td>\n",
       "      <td>1</td>\n",
       "      <td>[democratic, candidates, also, began, announci...</td>\n",
       "      <td>[democratic, candidate, also, began, announcin...</td>\n",
       "      <td>[democratic, candidates, also, began, announci...</td>\n",
       "      <td>[democratic, candidate, also, began, announcin...</td>\n",
       "      <td>0.909509</td>\n",
       "      <td>0.856008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_id_1</td>\n",
       "      <td>The woman was exposed to the SARS virus while ...</td>\n",
       "      <td>The woman was exposed to the SARS virus while ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[woman, exposed, sars, virus, hospital, health...</td>\n",
       "      <td>[woman, exposed, sars, virus, hospital, health...</td>\n",
       "      <td>[woman, exposed, sars, virus, hospital, health...</td>\n",
       "      <td>[woman, exposed, sars, virus, hospital, health...</td>\n",
       "      <td>0.904534</td>\n",
       "      <td>0.804030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_id_2</td>\n",
       "      <td>He said the problem needs to be corrected befo...</td>\n",
       "      <td>He said the prob lem needs to be corrected bef...</td>\n",
       "      <td>1</td>\n",
       "      <td>[said, problem, needs, corrected, space, shutt...</td>\n",
       "      <td>[said, problem, need, corrected, space, shuttl...</td>\n",
       "      <td>[said, prob, lem, needs, corrected, space, shu...</td>\n",
       "      <td>[said, prob, lem, need, corrected, space, shut...</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.777778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_id_3</td>\n",
       "      <td>A representative for Phoenix-based U-Haul decl...</td>\n",
       "      <td>Anthony Citrano , a representative for WhenU ,...</td>\n",
       "      <td>0</td>\n",
       "      <td>[representative, phoenix, based, u, haul, decl...</td>\n",
       "      <td>[representative, phoenix, based, u, haul, decl...</td>\n",
       "      <td>[anthony, citrano, representative, whenu, decl...</td>\n",
       "      <td>[anthony, citrano, representative, whenu, decl...</td>\n",
       "      <td>0.455842</td>\n",
       "      <td>0.290081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_id_4</td>\n",
       "      <td>The biggest threat to order seemed to be looti...</td>\n",
       "      <td>The biggest threat to order seemed to be looti...</td>\n",
       "      <td>1</td>\n",
       "      <td>[biggest, threat, order, seemed, looting, crim...</td>\n",
       "      <td>[biggest, threat, order, seemed, looting, crim...</td>\n",
       "      <td>[biggest, threat, order, seemed, looting, crim...</td>\n",
       "      <td>[biggest, threat, order, seemed, looting, crim...</td>\n",
       "      <td>0.721688</td>\n",
       "      <td>0.541266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4072</th>\n",
       "      <td>train_id_4072</td>\n",
       "      <td>Axelrod died in his sleep of heart failure , s...</td>\n",
       "      <td>Axelrod died of heart failure while asleep at ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[axelrod, died, sleep, heart, failure, said, d...</td>\n",
       "      <td>[axelrod, died, sleep, heart, failure, said, d...</td>\n",
       "      <td>[axelrod, died, heart, failure, asleep, los, a...</td>\n",
       "      <td>[axelrod, died, heart, failure, asleep, los, a...</td>\n",
       "      <td>0.805823</td>\n",
       "      <td>0.604367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4073</th>\n",
       "      <td>train_id_4073</td>\n",
       "      <td>Saddam 's other son , Odai , surrendered Frida...</td>\n",
       "      <td>Hussein 's other son , Uday , surrendered yest...</td>\n",
       "      <td>1</td>\n",
       "      <td>[saddam, son, odai, surrendered, friday, ameri...</td>\n",
       "      <td>[saddam, son, odai, surrendered, friday, ameri...</td>\n",
       "      <td>[hussein, son, uday, surrendered, yesterday, a...</td>\n",
       "      <td>[hussein, son, uday, surrendered, yesterday, a...</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4074</th>\n",
       "      <td>train_id_4074</td>\n",
       "      <td>If Senator Clinton does decide to run in 2008 ...</td>\n",
       "      <td>If Mrs Clinton does decide to contest the 2008...</td>\n",
       "      <td>1</td>\n",
       "      <td>[senator, clinton, decide, run, 2008, cannot, ...</td>\n",
       "      <td>[senator, clinton, decide, run, 2008, cannot, ...</td>\n",
       "      <td>[mrs, clinton, decide, contest, 2008, election...</td>\n",
       "      <td>[mr, clinton, decide, contest, 2008, election,...</td>\n",
       "      <td>0.819892</td>\n",
       "      <td>0.756823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4075</th>\n",
       "      <td>train_id_4075</td>\n",
       "      <td>The Iranian refugee who sewed up his eyes , li...</td>\n",
       "      <td>An Iranian Kurd who stitched up his eyes , lip...</td>\n",
       "      <td>1</td>\n",
       "      <td>[iranian, refugee, sewed, eyes, lips, ears, pr...</td>\n",
       "      <td>[iranian, refugee, sewed, eye, lip, ear, prote...</td>\n",
       "      <td>[iranian, kurd, stitched, eyes, lips, ears, pr...</td>\n",
       "      <td>[iranian, kurd, stitched, eye, lip, ear, prote...</td>\n",
       "      <td>0.560449</td>\n",
       "      <td>0.517337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4076</th>\n",
       "      <td>train_id_4076</td>\n",
       "      <td>Gemstar 's shares gathered up 2.6 percent , ad...</td>\n",
       "      <td>Gemstar shares moved higher on the news , clos...</td>\n",
       "      <td>1</td>\n",
       "      <td>[gemstar, shares, gathered, 2, 6, percent, add...</td>\n",
       "      <td>[gemstar, share, gathered, 2, 6, percent, addi...</td>\n",
       "      <td>[gemstar, shares, moved, higher, news, closing...</td>\n",
       "      <td>[gemstar, share, moved, higher, news, closing,...</td>\n",
       "      <td>0.583333</td>\n",
       "      <td>0.583333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4077 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                          sentence1  \\\n",
       "0        train_id_0  The Democratic candidates also began announcin...   \n",
       "1        train_id_1  The woman was exposed to the SARS virus while ...   \n",
       "2        train_id_2  He said the problem needs to be corrected befo...   \n",
       "3        train_id_3  A representative for Phoenix-based U-Haul decl...   \n",
       "4        train_id_4  The biggest threat to order seemed to be looti...   \n",
       "...             ...                                                ...   \n",
       "4072  train_id_4072  Axelrod died in his sleep of heart failure , s...   \n",
       "4073  train_id_4073  Saddam 's other son , Odai , surrendered Frida...   \n",
       "4074  train_id_4074  If Senator Clinton does decide to run in 2008 ...   \n",
       "4075  train_id_4075  The Iranian refugee who sewed up his eyes , li...   \n",
       "4076  train_id_4076  Gemstar 's shares gathered up 2.6 percent , ad...   \n",
       "\n",
       "                                              sentence2  classification  \\\n",
       "0     The Democratic candidates also began announcin...               1   \n",
       "1     The woman was exposed to the SARS virus while ...               1   \n",
       "2     He said the prob lem needs to be corrected bef...               1   \n",
       "3     Anthony Citrano , a representative for WhenU ,...               0   \n",
       "4     The biggest threat to order seemed to be looti...               1   \n",
       "...                                                 ...             ...   \n",
       "4072  Axelrod died of heart failure while asleep at ...               1   \n",
       "4073  Hussein 's other son , Uday , surrendered yest...               1   \n",
       "4074  If Mrs Clinton does decide to contest the 2008...               1   \n",
       "4075  An Iranian Kurd who stitched up his eyes , lip...               1   \n",
       "4076  Gemstar shares moved higher on the news , clos...               1   \n",
       "\n",
       "                                          Text_Cleaned1  \\\n",
       "0     [democratic, candidates, also, began, announci...   \n",
       "1     [woman, exposed, sars, virus, hospital, health...   \n",
       "2     [said, problem, needs, corrected, space, shutt...   \n",
       "3     [representative, phoenix, based, u, haul, decl...   \n",
       "4     [biggest, threat, order, seemed, looting, crim...   \n",
       "...                                                 ...   \n",
       "4072  [axelrod, died, sleep, heart, failure, said, d...   \n",
       "4073  [saddam, son, odai, surrendered, friday, ameri...   \n",
       "4074  [senator, clinton, decide, run, 2008, cannot, ...   \n",
       "4075  [iranian, refugee, sewed, eyes, lips, ears, pr...   \n",
       "4076  [gemstar, shares, gathered, 2, 6, percent, add...   \n",
       "\n",
       "                                       lemmatized_text1  \\\n",
       "0     [democratic, candidate, also, began, announcin...   \n",
       "1     [woman, exposed, sars, virus, hospital, health...   \n",
       "2     [said, problem, need, corrected, space, shuttl...   \n",
       "3     [representative, phoenix, based, u, haul, decl...   \n",
       "4     [biggest, threat, order, seemed, looting, crim...   \n",
       "...                                                 ...   \n",
       "4072  [axelrod, died, sleep, heart, failure, said, d...   \n",
       "4073  [saddam, son, odai, surrendered, friday, ameri...   \n",
       "4074  [senator, clinton, decide, run, 2008, cannot, ...   \n",
       "4075  [iranian, refugee, sewed, eye, lip, ear, prote...   \n",
       "4076  [gemstar, share, gathered, 2, 6, percent, addi...   \n",
       "\n",
       "                                          Text_Cleaned2  \\\n",
       "0     [democratic, candidates, also, began, announci...   \n",
       "1     [woman, exposed, sars, virus, hospital, health...   \n",
       "2     [said, prob, lem, needs, corrected, space, shu...   \n",
       "3     [anthony, citrano, representative, whenu, decl...   \n",
       "4     [biggest, threat, order, seemed, looting, crim...   \n",
       "...                                                 ...   \n",
       "4072  [axelrod, died, heart, failure, asleep, los, a...   \n",
       "4073  [hussein, son, uday, surrendered, yesterday, a...   \n",
       "4074  [mrs, clinton, decide, contest, 2008, election...   \n",
       "4075  [iranian, kurd, stitched, eyes, lips, ears, pr...   \n",
       "4076  [gemstar, shares, moved, higher, news, closing...   \n",
       "\n",
       "                                       lemmatized_text2  \\\n",
       "0     [democratic, candidate, also, began, announcin...   \n",
       "1     [woman, exposed, sars, virus, hospital, health...   \n",
       "2     [said, prob, lem, need, corrected, space, shut...   \n",
       "3     [anthony, citrano, representative, whenu, decl...   \n",
       "4     [biggest, threat, order, seemed, looting, crim...   \n",
       "...                                                 ...   \n",
       "4072  [axelrod, died, heart, failure, asleep, los, a...   \n",
       "4073  [hussein, son, uday, surrendered, yesterday, a...   \n",
       "4074  [mr, clinton, decide, contest, 2008, election,...   \n",
       "4075  [iranian, kurd, stitched, eye, lip, ear, prote...   \n",
       "4076  [gemstar, share, moved, higher, news, closing,...   \n",
       "\n",
       "      cosine_similarity_score  overall_similarity_score  \n",
       "0                    0.909509                  0.856008  \n",
       "1                    0.904534                  0.804030  \n",
       "2                    0.777778                  0.777778  \n",
       "3                    0.455842                  0.290081  \n",
       "4                    0.721688                  0.541266  \n",
       "...                       ...                       ...  \n",
       "4072                 0.805823                  0.604367  \n",
       "4073                 0.700000                  0.700000  \n",
       "4074                 0.819892                  0.756823  \n",
       "4075                 0.560449                  0.517337  \n",
       "4076                 0.583333                  0.583333  \n",
       "\n",
       "[4077 rows x 10 columns]"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosinelist = []\n",
    "similaritylist = []\n",
    "for index, row in train_df.iterrows():\n",
    "    #Cosine Similarity:\n",
    "    counter1 = Counter(row['lemmatized_text1'])\n",
    "    counter2 = Counter(row['lemmatized_text2'])\n",
    "    cosine_score = counter_cosine_similarity(counter1, counter2)\n",
    "    cosinelist.append(cosine_score)\n",
    "    \n",
    "    #Length Similarity:\n",
    "    lenc1 = sum(iter(counter1.values()))\n",
    "    lenc2 = sum(iter(counter2.values()))\n",
    "    lengthSim = min(lenc1, lenc2) / float(max(lenc1, lenc2))\n",
    "    \n",
    "    #Similarity Score:\n",
    "    similarityScore = lengthSim * cosine_score\n",
    "    similaritylist.append(similarityScore)\n",
    "train_df['cosine_similarity_score'] = cosinelist\n",
    "train_df['overall_similarity_score'] = similaritylist\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "1ddf8dce-fe80-4fe4-b36d-504abfe11cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_df = pd.read_csv(\"dev_with_label.txt\", delimiter = \"r'\\t\", header = None, engine = 'python')\n",
    "dev_df = dev_df[0].str.split(\"\\t\", expand=True)\n",
    "dev_df = dev_df.rename(columns={0: \"id\", 1: \"sentence1\", 2: \"sentence2\", 3: \"classification\"})\n",
    "dev_df[\"classification\"] = pd.to_numeric(dev_df[\"classification\"])\n",
    "dev_df.drop_duplicates(inplace = True)\n",
    "\n",
    "lemm = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "dev_df['Text_Cleaned1'] = list(map(clean_text, dev_df.sentence1))\n",
    "dev_df['lemmatized_text1'] = list(map(lambda word:list(map(lemm.lemmatize, word)),dev_df.Text_Cleaned1))\n",
    "dev_df['Text_Cleaned2'] = list(map(clean_text, dev_df.sentence2))\n",
    "dev_df['lemmatized_text2'] = list(map(lambda word:list(map(lemm.lemmatize, word)),dev_df.Text_Cleaned2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "5108cbac-46d5-4cbe-a225-679e3aa199c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>classification</th>\n",
       "      <th>Text_Cleaned1</th>\n",
       "      <th>lemmatized_text1</th>\n",
       "      <th>Text_Cleaned2</th>\n",
       "      <th>lemmatized_text2</th>\n",
       "      <th>cosine_similarity_score</th>\n",
       "      <th>overall_similarity_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dev_id_0</td>\n",
       "      <td>Local police authorities are treating the expl...</td>\n",
       "      <td>Acting New Haven Police Chief Francisco Ortiz ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[local, police, authorities, treating, explosi...</td>\n",
       "      <td>[local, police, authority, treating, explosion...</td>\n",
       "      <td>[acting, new, police, chief, francisco, ortiz,...</td>\n",
       "      <td>[acting, new, police, chief, francisco, ortiz,...</td>\n",
       "      <td>0.534522</td>\n",
       "      <td>0.400892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dev_id_1</td>\n",
       "      <td>The report shows that drugs sold in Canadian p...</td>\n",
       "      <td>The report shows that drugs sold in Canadian p...</td>\n",
       "      <td>1</td>\n",
       "      <td>[report, shows, drugs, sold, canadian, pharmac...</td>\n",
       "      <td>[report, show, drug, sold, canadian, pharmacy,...</td>\n",
       "      <td>[report, shows, drugs, sold, canadian, pharmac...</td>\n",
       "      <td>[report, show, drug, sold, canadian, pharmacy,...</td>\n",
       "      <td>0.802955</td>\n",
       "      <td>0.661257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dev_id_2</td>\n",
       "      <td>The transition is slated to begin no later tha...</td>\n",
       "      <td>A two-week transition period will begin no lat...</td>\n",
       "      <td>1</td>\n",
       "      <td>[transition, slated, begin, later, june, 7, da...</td>\n",
       "      <td>[transition, slated, begin, later, june, 7, da...</td>\n",
       "      <td>[two, week, transition, period, begin, later, ...</td>\n",
       "      <td>[two, week, transition, period, begin, later, ...</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.625000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dev_id_3</td>\n",
       "      <td>Like Viacom , GE -- parent of NBC -- is also s...</td>\n",
       "      <td>Like Viacom , General Electric is seen as a le...</td>\n",
       "      <td>1</td>\n",
       "      <td>[like, viacom, ge, parent, nbc, also, seen, le...</td>\n",
       "      <td>[like, viacom, ge, parent, nbc, also, seen, le...</td>\n",
       "      <td>[like, viacom, general, electric, seen, less, ...</td>\n",
       "      <td>[like, viacom, general, electric, seen, le, en...</td>\n",
       "      <td>0.753778</td>\n",
       "      <td>0.592254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dev_id_4</td>\n",
       "      <td>Last month , 62 Spanish peacekeepers died when...</td>\n",
       "      <td>In another disaster , 62 Spanish peacekeepers ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[last, month, 62, spanish, peacekeepers, died,...</td>\n",
       "      <td>[last, month, 62, spanish, peacekeeper, died, ...</td>\n",
       "      <td>[another, disaster, 62, spanish, peacekeepers,...</td>\n",
       "      <td>[another, disaster, 62, spanish, peacekeeper, ...</td>\n",
       "      <td>0.585369</td>\n",
       "      <td>0.495313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>dev_id_719</td>\n",
       "      <td>He is a brother to three-year-old Mia , from K...</td>\n",
       "      <td>Winslet , 28 , has a three-year-old daughter M...</td>\n",
       "      <td>0</td>\n",
       "      <td>[brother, three, year, old, mia, kate, first, ...</td>\n",
       "      <td>[brother, three, year, old, mia, kate, first, ...</td>\n",
       "      <td>[winslet, 28, three, year, old, daughter, mia,...</td>\n",
       "      <td>[winslet, 28, three, year, old, daughter, mia,...</td>\n",
       "      <td>0.694365</td>\n",
       "      <td>0.595170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>720</th>\n",
       "      <td>dev_id_720</td>\n",
       "      <td>Some 175 million shares traded on the Big Boar...</td>\n",
       "      <td>Some 1.6 billion shares traded on the Big Boar...</td>\n",
       "      <td>0</td>\n",
       "      <td>[175, million, shares, traded, big, board, 7, ...</td>\n",
       "      <td>[175, million, share, traded, big, board, 7, p...</td>\n",
       "      <td>[1, 6, billion, shares, traded, big, board, 17...</td>\n",
       "      <td>[1, 6, billion, share, traded, big, board, 17,...</td>\n",
       "      <td>0.462910</td>\n",
       "      <td>0.396780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>721</th>\n",
       "      <td>dev_id_721</td>\n",
       "      <td>Mr Berlusconi is accused of bribing judges to ...</td>\n",
       "      <td>Mr Berlusconi is accused of bribing judges to ...</td>\n",
       "      <td>1</td>\n",
       "      <td>[mr, berlusconi, accused, bribing, judges, inf...</td>\n",
       "      <td>[mr, berlusconi, accused, bribing, judge, infl...</td>\n",
       "      <td>[mr, berlusconi, accused, bribing, judges, inf...</td>\n",
       "      <td>[mr, berlusconi, accused, bribing, judge, infl...</td>\n",
       "      <td>0.716115</td>\n",
       "      <td>0.620633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>722</th>\n",
       "      <td>dev_id_722</td>\n",
       "      <td>He added that those \" are not solely American ...</td>\n",
       "      <td>\" These are not solely American principles nor...</td>\n",
       "      <td>1</td>\n",
       "      <td>[added, solely, american, principles, exclusiv...</td>\n",
       "      <td>[added, solely, american, principle, exclusive...</td>\n",
       "      <td>[solely, american, principles, exclusively, we...</td>\n",
       "      <td>[solely, american, principle, exclusively, wes...</td>\n",
       "      <td>0.771517</td>\n",
       "      <td>0.661300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>723</th>\n",
       "      <td>dev_id_723</td>\n",
       "      <td>Memories also live on of the bloody debacle in...</td>\n",
       "      <td>Memories also live on of a bloody debacle in S...</td>\n",
       "      <td>1</td>\n",
       "      <td>[memories, also, live, bloody, debacle, somali...</td>\n",
       "      <td>[memory, also, live, bloody, debacle, somalia,...</td>\n",
       "      <td>[memories, also, live, bloody, debacle, somali...</td>\n",
       "      <td>[memory, also, live, bloody, debacle, somalia,...</td>\n",
       "      <td>0.897085</td>\n",
       "      <td>0.837280</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>724 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             id                                          sentence1  \\\n",
       "0      dev_id_0  Local police authorities are treating the expl...   \n",
       "1      dev_id_1  The report shows that drugs sold in Canadian p...   \n",
       "2      dev_id_2  The transition is slated to begin no later tha...   \n",
       "3      dev_id_3  Like Viacom , GE -- parent of NBC -- is also s...   \n",
       "4      dev_id_4  Last month , 62 Spanish peacekeepers died when...   \n",
       "..          ...                                                ...   \n",
       "719  dev_id_719  He is a brother to three-year-old Mia , from K...   \n",
       "720  dev_id_720  Some 175 million shares traded on the Big Boar...   \n",
       "721  dev_id_721  Mr Berlusconi is accused of bribing judges to ...   \n",
       "722  dev_id_722  He added that those \" are not solely American ...   \n",
       "723  dev_id_723  Memories also live on of the bloody debacle in...   \n",
       "\n",
       "                                             sentence2  classification  \\\n",
       "0    Acting New Haven Police Chief Francisco Ortiz ...               0   \n",
       "1    The report shows that drugs sold in Canadian p...               1   \n",
       "2    A two-week transition period will begin no lat...               1   \n",
       "3    Like Viacom , General Electric is seen as a le...               1   \n",
       "4    In another disaster , 62 Spanish peacekeepers ...               1   \n",
       "..                                                 ...             ...   \n",
       "719  Winslet , 28 , has a three-year-old daughter M...               0   \n",
       "720  Some 1.6 billion shares traded on the Big Boar...               0   \n",
       "721  Mr Berlusconi is accused of bribing judges to ...               1   \n",
       "722  \" These are not solely American principles nor...               1   \n",
       "723  Memories also live on of a bloody debacle in S...               1   \n",
       "\n",
       "                                         Text_Cleaned1  \\\n",
       "0    [local, police, authorities, treating, explosi...   \n",
       "1    [report, shows, drugs, sold, canadian, pharmac...   \n",
       "2    [transition, slated, begin, later, june, 7, da...   \n",
       "3    [like, viacom, ge, parent, nbc, also, seen, le...   \n",
       "4    [last, month, 62, spanish, peacekeepers, died,...   \n",
       "..                                                 ...   \n",
       "719  [brother, three, year, old, mia, kate, first, ...   \n",
       "720  [175, million, shares, traded, big, board, 7, ...   \n",
       "721  [mr, berlusconi, accused, bribing, judges, inf...   \n",
       "722  [added, solely, american, principles, exclusiv...   \n",
       "723  [memories, also, live, bloody, debacle, somali...   \n",
       "\n",
       "                                      lemmatized_text1  \\\n",
       "0    [local, police, authority, treating, explosion...   \n",
       "1    [report, show, drug, sold, canadian, pharmacy,...   \n",
       "2    [transition, slated, begin, later, june, 7, da...   \n",
       "3    [like, viacom, ge, parent, nbc, also, seen, le...   \n",
       "4    [last, month, 62, spanish, peacekeeper, died, ...   \n",
       "..                                                 ...   \n",
       "719  [brother, three, year, old, mia, kate, first, ...   \n",
       "720  [175, million, share, traded, big, board, 7, p...   \n",
       "721  [mr, berlusconi, accused, bribing, judge, infl...   \n",
       "722  [added, solely, american, principle, exclusive...   \n",
       "723  [memory, also, live, bloody, debacle, somalia,...   \n",
       "\n",
       "                                         Text_Cleaned2  \\\n",
       "0    [acting, new, police, chief, francisco, ortiz,...   \n",
       "1    [report, shows, drugs, sold, canadian, pharmac...   \n",
       "2    [two, week, transition, period, begin, later, ...   \n",
       "3    [like, viacom, general, electric, seen, less, ...   \n",
       "4    [another, disaster, 62, spanish, peacekeepers,...   \n",
       "..                                                 ...   \n",
       "719  [winslet, 28, three, year, old, daughter, mia,...   \n",
       "720  [1, 6, billion, shares, traded, big, board, 17...   \n",
       "721  [mr, berlusconi, accused, bribing, judges, inf...   \n",
       "722  [solely, american, principles, exclusively, we...   \n",
       "723  [memories, also, live, bloody, debacle, somali...   \n",
       "\n",
       "                                      lemmatized_text2  \\\n",
       "0    [acting, new, police, chief, francisco, ortiz,...   \n",
       "1    [report, show, drug, sold, canadian, pharmacy,...   \n",
       "2    [two, week, transition, period, begin, later, ...   \n",
       "3    [like, viacom, general, electric, seen, le, en...   \n",
       "4    [another, disaster, 62, spanish, peacekeeper, ...   \n",
       "..                                                 ...   \n",
       "719  [winslet, 28, three, year, old, daughter, mia,...   \n",
       "720  [1, 6, billion, share, traded, big, board, 17,...   \n",
       "721  [mr, berlusconi, accused, bribing, judge, infl...   \n",
       "722  [solely, american, principle, exclusively, wes...   \n",
       "723  [memory, also, live, bloody, debacle, somalia,...   \n",
       "\n",
       "     cosine_similarity_score  overall_similarity_score  \n",
       "0                   0.534522                  0.400892  \n",
       "1                   0.802955                  0.661257  \n",
       "2                   0.625000                  0.625000  \n",
       "3                   0.753778                  0.592254  \n",
       "4                   0.585369                  0.495313  \n",
       "..                       ...                       ...  \n",
       "719                 0.694365                  0.595170  \n",
       "720                 0.462910                  0.396780  \n",
       "721                 0.716115                  0.620633  \n",
       "722                 0.771517                  0.661300  \n",
       "723                 0.897085                  0.837280  \n",
       "\n",
       "[724 rows x 10 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cosinelist = []\n",
    "similaritylist = []\n",
    "for index, row in dev_df.iterrows():\n",
    "    #Cosine Similarity:\n",
    "    counter1 = Counter(row['lemmatized_text1'])\n",
    "    counter2 = Counter(row['lemmatized_text2'])\n",
    "    cosine_score = counter_cosine_similarity(counter1, counter2)\n",
    "    cosinelist.append(cosine_score)\n",
    "    \n",
    "    #Length Similarity:\n",
    "    lenc1 = sum(iter(counter1.values()))\n",
    "    lenc2 = sum(iter(counter2.values()))\n",
    "    lengthSim = min(lenc1, lenc2) / float(max(lenc1, lenc2))\n",
    "    \n",
    "    #Similarity Score:\n",
    "    similarityScore = lengthSim * cosine_score\n",
    "    similaritylist.append(similarityScore)\n",
    "dev_df['cosine_similarity_score'] = cosinelist\n",
    "dev_df['overall_similarity_score'] = similaritylist\n",
    "dev_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e47ddfca-94ef-46cc-8a8b-67cad9c55080",
   "metadata": {},
   "source": [
    "### Logistic Regression Model 1: Using just Similarity Score:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b75d385c-84c2-4580-8c2a-243803e740f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 4077, val: 724\n",
      "(4077, 1)\n",
      "(724, 1)\n"
     ]
    }
   ],
   "source": [
    "x_train = train_df[['cosine_similarity_score']]\n",
    "y_train = train_df['classification'].values\n",
    "x_train_val = dev_df[['cosine_similarity_score']]\n",
    "y_train_val = dev_df['classification'].values\n",
    "\n",
    "print(\"train: {}, val: {}\".format(x_train.shape[0], x_train_val.shape[0]))\n",
    "print(x_train.shape)\n",
    "print(x_train_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "944a6931-3d4c-4ae3-b58e-8f39b39e9c2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score with Cosine Similarity Score features 0.5814917127071824\n"
     ]
    }
   ],
   "source": [
    "model_simple = simple_logistic_classify(x_train, y_train, x_train_val, y_train_val, 'Cosine Similarity Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb95fe2-39ce-4766-9656-4ff1cf000eeb",
   "metadata": {},
   "source": [
    "### Logistic Regression Models 2 and 3: Using bow and tf-idf features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "4f358646-7da6-4abe-98b8-fa4110a003cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13042"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_combinedCols_train = train_df['Text_Cleaned1'] + train_df['Text_Cleaned2']\n",
    "x_combinedCols_val = dev_df['Text_Cleaned1'] + dev_df['Text_Cleaned2']\n",
    "\n",
    "bow_converter = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False)\n",
    "x = bow_converter.fit_transform(x_combinedCols_train)\n",
    "\n",
    "words = bow_converter.get_feature_names()\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "943c604b-2038-4cc5-a615-25f2c3b551f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60828"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_converter = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[2,2], lowercase=False) \n",
    "x2 = bigram_converter.fit_transform(x_combinedCols_train)\n",
    "bigrams = bigram_converter.get_feature_names()\n",
    "len(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "1f42a086-6adc-48d8-8351-84a6e872bcb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71562"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_converter = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[3,3], lowercase=False) \n",
    "x3 = trigram_converter.fit_transform(x_combinedCols_train)\n",
    "trigrams = trigram_converter.get_feature_names()\n",
    "len(trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5cebb39b-d67f-410e-95c1-c314c304a1e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73786"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quadgram_converter = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[4,4], lowercase=False) \n",
    "x3 = quadgram_converter.fit_transform(x_combinedCols_train)\n",
    "quadgrams = quadgram_converter.get_feature_names()\n",
    "len(quadgrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "94560bdd-8716-42d3-bf2b-0cb1c9f7441d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "73687"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fivegram_converter = CountVectorizer(tokenizer=lambda doc: doc, ngram_range=[5,5], lowercase=False) \n",
    "x3 = fivegram_converter.fit_transform(x_combinedCols_train)\n",
    "fivegrams = fivegram_converter.get_feature_names()\n",
    "len(fivegrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "59c930ce-e972-4f9a-ac96-ace2d850968b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train: 4077, val: 724\n",
      "(4077,)\n",
      "(724,)\n"
     ]
    }
   ],
   "source": [
    "# x_train = train_df[['Text_Cleaned1']].values\n",
    "# y_train = train_df['classification'].values\n",
    "# x_train_val = dev_df[['Text_Cleaned1', 'Text_Cleaned2']].values\n",
    "# y_train_val = dev_df['classification'].values\n",
    "\n",
    "x_train = x_combinedCols_train.values\n",
    "y_train = train_df['classification'].values\n",
    "x_train_val = x_combinedCols_val.values\n",
    "y_train_val = dev_df['classification'].values\n",
    "\n",
    "print(\"train: {}, val: {}\".format(x_train.shape[0], x_train_val.shape[0]))\n",
    "print(x_train.shape)\n",
    "print(x_train_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "de3bf42a-3ae7-4196-a5e9-1ecc2a4d5599",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13042"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_transform = CountVectorizer(tokenizer=lambda doc: doc, lowercase=False, ngram_range=[1,1])\n",
    "X_tr_bow = bow_transform.fit_transform(x_train)\n",
    "X_te_bow = bow_transform.transform(x_train_val)\n",
    "len(bow_transform.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "265aee01-cb1f-4279-a060-9f9384519c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tf-Idf Transformation:\n",
    "tfidf_transform = text.TfidfTransformer(norm=None)\n",
    "X_tr_tfidf = tfidf_transform.fit_transform(X_tr_bow)\n",
    "X_te_tfidf = tfidf_transform.transform(X_te_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "d7f4fd08-c859-49f7-b206-7d84e4721229",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Score with bow features 0.606353591160221\n",
      "Test Score with tf-idf features 0.5953038674033149\n"
     ]
    }
   ],
   "source": [
    "model_bow = simple_logistic_classify(X_tr_bow, y_train, X_te_bow, y_train_val, 'bow')\n",
    "model_tfidf = simple_logistic_classify(X_tr_tfidf, y_train, X_te_tfidf, y_train_val, 'tf-idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "37241a56-f8ec-429b-b1ac-970395c14e97",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1082 3902 4010 ...   82 1919 3978]\n",
      " [2484  566  717 ... 1031 3427 1207]\n",
      " [1351 3459 1572 ... 2117 4029 2607]\n",
      " ...\n",
      " [1085 3641   99 ...  375 2251 2706]\n",
      " [2167 2185 3339 ... 2818  932 1583]\n",
      " [2989 1313   57 ...  357 1096  214]]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: \"If Walker appeals Parrish 's ruling , it would stop the extradition process and could take several months , Rork said .\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-4a2dc9afb507>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m#Train the model with the training set:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 50\u001b[1;33m         \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     51\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0my_valid_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1342\u001b[0m             \u001b[0m_dtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1343\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1344\u001b[1;33m         X, y = self._validate_data(X, y, accept_sparse='csr', dtype=_dtype,\n\u001b[0m\u001b[0;32m   1345\u001b[0m                                    \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1346\u001b[0m                                    accept_large_sparse=solver != 'liblinear')\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    431\u001b[0m                 \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    432\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 433\u001b[1;33m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    434\u001b[0m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m    812\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"y cannot be None\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 814\u001b[1;33m     X = check_array(X, accept_sparse=accept_sparse,\n\u001b[0m\u001b[0;32m    815\u001b[0m                     \u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maccept_large_sparse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    816\u001b[0m                     \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator)\u001b[0m\n\u001b[0;32m    614\u001b[0m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcasting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"unsafe\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    615\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 616\u001b[1;33m                     \u001b[0marray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    617\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mComplexWarning\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcomplex_warning\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m                 raise ValueError(\"Complex data not supported\\n\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order, like)\u001b[0m\n\u001b[0;32m    100\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 102\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: \"If Walker appeals Parrish 's ruling , it would stop the extradition process and could take several months , Rork said .\""
     ]
    }
   ],
   "source": [
    "#We need to learn the model parameter  𝐰 . \n",
    "#However, with different hyperparameters  𝜆 , we can get different model parameter  𝐰 , resulting in different prediction performance. \n",
    "#Thus, we will use the 10-fold cross-validation to select the hyperparameter  𝜆 .\n",
    "\n",
    "#Here we set the folds equal to 10 for 10-fold cross-validation\n",
    "folds = 9\n",
    "\n",
    "#We get the number of samples in the training and validation set\n",
    "num_train = x_train.shape[0] \n",
    "\n",
    "#Now, we shuffle the index of samples in the train_val set\n",
    "index_of_samples = np.arange(num_train) \n",
    "shuffle(index_of_samples)\n",
    "\n",
    "#We split the index of the train_valid set into 10 folds\n",
    "index_of_folds = index_of_samples.reshape(folds, -1)\n",
    "print(index_of_folds)\n",
    "\n",
    "#As suggested above, the hyperparameters chosen are listed below\n",
    "regularization_coefficient = [10**(-5), 10**(-3), 10**(-2), 10**(-1), 1, 10, 20, 50]\n",
    "\n",
    "#Variables we create to store the values of the best accuracy and best regression:\n",
    "best_acc = 0.0\n",
    "best_reg = 0.0\n",
    "\n",
    "for reg in regularization_coefficient:\n",
    "    #10-fold cross-validation\n",
    "    sum_acc = 0.0\n",
    "    for fold in range(folds):\n",
    "        \n",
    "        index_of_folds_temp = index_of_folds.copy()\n",
    "        \n",
    "        #We are getting the index of the validation set and storing it in a variable valid_index\n",
    "        valid_index = index_of_folds_temp[fold,:].reshape(-1) \n",
    "        #We are getting the index of the training set and storing it in a variable train_index\n",
    "        train_index = np.delete(index_of_folds_temp, fold, 0).reshape(-1)\n",
    "        \n",
    "        #Our training set:\n",
    "        X_train = x_train[train_index]\n",
    "        Y_train = y_train[train_index]\n",
    "        \n",
    "        #Our validation set:\n",
    "        X_valid = x_train[valid_index]\n",
    "        Y_valid = y_train[valid_index]\n",
    "                \n",
    "        #We write this to build the model with different hyperparameters:\n",
    "        clf = LogisticRegression(penalty='l2', C=reg, solver='lbfgs')\n",
    "        \n",
    "        #Train the model with the training set:\n",
    "        clf.fit(X_train, Y_train)\n",
    "        \n",
    "        y_valid_pred = clf.predict(X_valid)\n",
    "        acc = accuracy_score(Y_valid, y_valid_pred)\n",
    "        \n",
    "        sum_acc += acc\n",
    "    \n",
    "    cur_acc = sum_acc / folds\n",
    "    \n",
    "    print(\"reg_coeff: {}, acc: {:.3f}\".format(1.0/reg, cur_acc))\n",
    "    \n",
    "    #We now want to store the best hyperparameter:\n",
    "    if cur_acc > best_acc:\n",
    "        best_acc = cur_acc\n",
    "        best_reg = reg\n",
    "        \n",
    "print(\"Best Accuracy: {:.4f} \".format(best_acc))\n",
    "print(\"Best Reg: {:}\".format(best_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ba63e48-dc56-4964-9cbe-354546ad2d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f9d0dd-b526-4ddb-b67b-c1c2902037df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalizing the features:\n",
    "normalizer = StandardScaler()\n",
    "X_train_val = normalizer.fit_transform(X_train_val)\n",
    "X_test = normalizer.transform(X_test)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(X_train_val.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
